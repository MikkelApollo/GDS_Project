{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleantext import clean\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,3] = word_tokenize(clean(re.sub(r'[a-zA-Z]{3,9}\\.? ?\\d{1,2}(,|.) ?\\d{2,4}', 'DATE', str(df.iat[i,3])), no_urls=True, no_emails=True, no_numbers=True, no_punct=True, replace_with_number='NUM', ))\n",
    "\n",
    "def remove_stopwords(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,3] = [w for w in df.iat[i,3] if w not in stop_words]\n",
    "\n",
    "def stem_words(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,3] = [stemmer.stem(word) for word in df.iat[i,3]]\n",
    "\n",
    "def lemmatize_words(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,3] = [lemmatizer.lemmatize(word) for word in df.iat[i,3]]\n",
    "\n",
    "def get_unique(df):\n",
    "  unique = []\n",
    "  for i in range(df.shape[0]):\n",
    "    for word in df.iat[i,3]:\n",
    "      if word not in unique:\n",
    "        unique.append(word)\n",
    "  return unique\n",
    "\n",
    "def process_data(df, lemma=False):\n",
    "  clean_tokens(df)\n",
    "  remove_stopwords(df)\n",
    "  if lemma:\n",
    "    lemmatize_words(df)\n",
    "  else:\n",
    "    stem_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc_stem_sample = pd.read_csv('FakeNewsCorpus_Sample.csv', usecols=[*range(2,16)])\n",
    "\n",
    "clean_tokens(fnc_stem_sample)\n",
    "unique_clean = get_unique(fnc_stem_sample)\n",
    "\n",
    "remove_stopwords(fnc_stem_sample)\n",
    "unique_no_stops = get_unique(fnc_stem_sample)\n",
    "\n",
    "fnc_lemma_sample = fnc_stem_sample.copy()\n",
    "\n",
    "stem_words(fnc_stem_sample)\n",
    "lemmatize_words(fnc_lemma_sample)\n",
    "unique_stemmed = get_unique(fnc_stem_sample)\n",
    "unique_lemmatized = get_unique(fnc_lemma_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in cleaned contents           : 16659\n",
      "Number of unique words after also removing stopwords : 16527\n",
      "Number of unique words after also stemming words     : 11004\n",
      "Number of unique words after lemmatizing instead     : 14618\n",
      "Reduction rate from removing stopwords    :  0.7923644876643255\n",
      "Further reduction rate from stemming      : 33.41804320203304\n",
      "Alternate reduction rate from lemmatizing : 11.550795667695287\n"
     ]
    }
   ],
   "source": [
    "red_rate_stops = (len(unique_clean) - len(unique_no_stops)) / len(unique_clean) * 100\n",
    "red_rate_stems = (len(unique_no_stops) - len(unique_stemmed)) / len(unique_no_stops) * 100\n",
    "red_rate_lemmas = (len(unique_no_stops) - len(unique_lemmatized)) / len(unique_no_stops) * 100\n",
    "\n",
    "print('Number of unique words in cleaned contents           : {}'.format(len(unique_clean)))\n",
    "print('Number of unique words after also removing stopwords : {}'.format(len(unique_no_stops)))\n",
    "print('Number of unique words after also stemming words     : {}'.format(len(unique_stemmed)))\n",
    "print('Number of unique words after lemmatizing instead     : {}'.format(len(unique_lemmatized)))\n",
    "print('Reduction rate from removing stopwords    :  {}'.format(red_rate_stops))\n",
    "print('Further reduction rate from stemming      : {}'.format(red_rate_stems))\n",
    "print('Alternate reduction rate from lemmatizing : {}'.format(red_rate_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnc_lemma = pd.read_csv('995,000_rows.csv', usecols=[*range(2,16)])\n",
    "# rnc_lemma = pd.read_csv('ReliableNewsCorpus.csv', usecols=[*range(1,14)])\n",
    "# process_data(rnc_lemma, lemma=True)\n",
    "# process_data(fnc_lemma, lemma=True)\n",
    "# data = pd.concat(fnc_lemma, rnc_lemma)\n",
    "# data.to_csv('dataset_lemmatized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
