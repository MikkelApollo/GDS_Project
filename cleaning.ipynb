{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleantext import clean\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_tokens(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,4] = word_tokenize(clean(re.sub(r'[a-zA-Z]{3,9}\\.? ?\\d{1,2}(,|.) ?\\d{2,4}', 'DATE', df.iat[i,4]), no_urls=True, no_emails=True, no_numbers=True, no_punct=True, replace_with_number='NUM'))\n",
    "\n",
    "def remove_stopwords(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,4] = [w for w in df.iat[i,4] if w not in stop_words]\n",
    "\n",
    "def stem_words(df):\n",
    "  for i in range(df.shape[0]):\n",
    "    df.iat[i,4] = [stemmer.stem(word) for word in df.iat[i,4]]\n",
    "\n",
    "def get_unique(df):\n",
    "  unique = []\n",
    "  for i in range(df.shape[0]):\n",
    "    for word in df.iat[i,4]:\n",
    "      if word not in unique:\n",
    "        unique.append(word)\n",
    "  return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc_sample = pd.read_csv('FakeNewsCorpus_Sample.csv', usecols=[*range(1,15)])\n",
    "\n",
    "clean_tokens(fnc_sample)\n",
    "unique_clean = get_unique(fnc_sample)\n",
    "\n",
    "remove_stopwords(fnc_sample)\n",
    "unique_no_stops = get_unique(fnc_sample)\n",
    "\n",
    "stem_words(fnc_sample)\n",
    "unique_stemmed = get_unique(fnc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in cleaned contents           : 16659\n",
      "Number of unique words after also removing stopwords : 16527\n",
      "Number of unique words after also stemming words     : 11004\n",
      "Reduction rate from removing stopwords :  0.7923644876643255\n",
      "Further reduction rate from stemming   : 33.41804320203304\n"
     ]
    }
   ],
   "source": [
    "red_rate_stops = (len(unique_clean) - len(unique_no_stops)) / len(unique_clean) * 100\n",
    "red_rate_stems = (len(unique_no_stops) - len(unique_stemmed)) / len(unique_no_stops) * 100\n",
    "\n",
    "print('Number of unique words in cleaned contents           : {}'.format(len(unique_clean)))\n",
    "print('Number of unique words after also removing stopwords : {}'.format(len(unique_no_stops)))\n",
    "print('Number of unique words after also stemming words     : {}'.format(len(unique_stemmed)))\n",
    "print('Reduction rate from removing stopwords :  {}'.format(red_rate_stops))\n",
    "print('Further reduction rate from stemming   : {}'.format(red_rate_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rare', 'shark', 'caught', 'scientist', 'left', 'blunder', 'answer', 'shark', 'uniqu', 'featur', 'surviv', 'extrem', 'depth', 'live', 'extend', 'period', 'time', 'shark', 'uniqu', 'trait', 'extend', 'jaw', 'telescop', 'fashion', 'beyond', 'mouth', 'youv', 'ever', 'seen', 'hit', 'scienc', 'fiction', 'movi', 'alien', 'shark', 'monster', 'made', 'manifest', 'earth', 'shark', 'stalk', 'prey', 'lash', 'telescop', 'teeth', 'snatch', 'larg', 'fish', 'swallow', 'one', 'bite', 'dead', 'predat', 'deep', 'sea', 'besid', 'telescop', 'teeth', 'shark', 'glow', 'dark', 'recent', 'discov', 'back', 'num', 'three', 'decad', 'sinc', 'first', 'found', 'select', 'exot', 'shark', 'speci', 'found', 'routin', 'survey', 'dongh', 'township', 'taiwan', 'countri', 'fisheri', 'research', 'institut', 'found', 'five', 'horrifi', 'deepsea', 'shark', 'team', 'notabl', 'horrifi', 'describ', 'obvious', 'featur', 'needleshap', 'teeth', 'like', 'snakelik', 'fang', 'also', 'origin', 'viper', 'shark', 'name', 'that', 'right', 'beast', 'call', 'viper', 'shark', 'tell', 'name', 'feroci', 'prey', 'although', 'research', 'gotten', 'hand', 'anim', 'littl', 'known', 'research', 'think', 'creatur', 'swim', 'numnum', 'meter', 'surfac', 'ocean', 'day', 'num', 'meter', 'night', 'could', 'temperatur', 'chang', 'someth', 'light', 'last', 'five', 'specimen', 'trawl', 'depth', 'num', 'meter', 'surfac', 'sea', 'four', 'alreadi', 'dead', 'live', 'specimen', 'immedi', 'immers', 'cool', 'seawat', 'could', 'handl', 'sudden', 'chang', 'habitat', 'perish', 'day', 'later', 'viper', 'shark', 'look', 'like', 'alien', 'monster', 'movi', 'feed', 'human', 'main', 'food', 'sourc', 'crustacean', 'boni', 'fish', 'one', 'favorit', 'lunch', 'item', 'lanternfish', 'creatur', 'might', 'attract', 'toward', 'shark', 'glowinthedark', 'bodi', 'swim', 'close', 'enough', 'shark', 'lash', 'extend', 'jaw', 'snatch', 'fish', 'devour', 'whole', 'first', 'specimen', 'viper', 'shark', 'found', 'coast', 'shikoku', 'island', 'japan', 'num', 'bottomtrawl', 'vessel', 'seiryomaru', 'first', 'haul', 'creatur', 'surfac', 'sea', 'fish', 'new', 'discoveri', 'time', 'scientif', 'name', 'honor', 'seiryomaru', 'captain', 'hiromichi', 'kabeya', 'shark', 'scientif', 'name', 'trigonognathus', 'kabeyai', 'although', 'much', 'fish', 'known', 'scientist', 'classifi', 'dogfish', 'shark', 'found', 'deep', 'part', 'pacif', 'ocean', 'would', 'react', 'shark', 'came', 'end', 'fish', 'line', 'would', 'think', 'nightmar']\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "print(fnc_sample.iat[3,4])\n",
    "print(len(fnc_sample.iat[3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fnc_sample.to_csv('FNC_clean.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
